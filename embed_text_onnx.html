<!DOCTYPE html>
<html>
  <head>
    <title>Web/JS LiT Text Embedding Demo with ONNX Web Runtime</title>
  </head>
  <body>
    <script src="./enable-threads.js"></script> <!-- a hack to add COEP/COOP headers which are needed for `SharedArrayBuffer`s and thread. Normally you would add these headers when serving this HTML file, but Github Pages doesn't allow setting these headers as of writing -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.11.0/dist/ort.js"></script>
    
    <div>See browser console for model download and inference progress.</div>
    <p><a href="https://github.com/josephrocca/lit-encoder-js">github repo</a> - <a href="https://huggingface.co/rocca/lit-web/tree/main">huggingface repo</a></p>
    
    <script>
      if(self.crossOriginIsolated) { // needs to be cross-origin-isolated to use wasm threads. you need to serve this html file with these two headers: https://web.dev/coop-coep/
        ort.env.wasm.numThreads = navigator.hardwareConcurrency / 2;
      }
      
      (async function() {
        console.log("Downloading model... (see network tab for progress)");
        let modelPath = 'https://huggingface.co/rocca/lit-web/resolve/main/embed_text_tokens.onnx';
        const session = await ort.InferenceSession.create(modelPath, { executionProviders: ["wasm"] });
        console.log("Model loaded.");
        
        let texts = ["hello world!", "testing one two three", "hi", "aaaaaaaaaaaaaaaa"];
        // TODO: Add the tokenizer here. Waiting for response here: https://twitter.com/rocca27/status/1525406593617391617
        
        // Tokens computed in Python notebook:
        let textTokens = [[101,7592,2088,999,0,0,0,0,0,0,0,0,0,0,0,0],[101,5604,2028,2048,2093,0,0,0,0,0,0,0,0,0,0,0],[101,7632,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[101,13360,11057,11057,11057,11057,11057,11057,2050,0,0,0,0,0,0,0]];
        
        textTokens = Int32Array.from(textTokens[0]); // just use the first one for this test ("hello world!")
        
        const feeds = {'text_tokens': new ort.Tensor('int32', textTokens, [1,16])};

        console.log("Running inference...");
        const results = await session.run(feeds);
        console.log("Finished inference.");

        const data = results["Identity_1:0"].data;
        console.log(`data of result tensor 'output'`, data); // Should match this Python output (all 4 `texts`): <tf.Tensor: shape=(4, 768), dtype=float32, numpy= array([[-0.00325142,  0.01848885,  0.00041978, ...,  0.00173793, -0.00536559, -0.06884101], [-0.01331316,  0.01797788,  0.00657475, ...,  0.00655348, 0.02183297, -0.08194969], [ 0.00275494,  0.0236993 ,  0.0011628 , ...,  0.00860824, 0.00814609, -0.05825125], [ 0.00415286,  0.00938824, -0.00084121, ...,  0.01578477, 0.0134029 , -0.06656414]], dtype=float32)>
      })();
    </script>
  </body>
</html>
