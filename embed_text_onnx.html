<!DOCTYPE html>
<html>
  <head>
    <title>Web/JS LiT Text Embedding Demo with ONNX Web Runtime</title>
  </head>
  <body>
    <script src="./enable-threads.js"></script> <!-- a hack to add COEP/COOP headers which are needed for `SharedArrayBuffer`s and thread. Normally you would add these headers when serving this HTML file, but Github Pages doesn't allow setting these headers as of writing -->
    <script src="https://cdn.jsdelivr.net/npm/onnxruntime-web@1.11.0/dist/ort.js"></script>
    
    <div>See browser console for model download and inference progress.</div>
    <p><a href="https://github.com/josephrocca/lit-encoder-js">github repo</a> - <a href="https://huggingface.co/rocca/lit-web/tree/main">huggingface repo</a></p>
    
    <script>
      if(self.crossOriginIsolated) { // needs to be cross-origin-isolated to use wasm threads. you need to serve this html file with these two headers: https://web.dev/coop-coep/
        ort.env.wasm.numThreads = navigator.hardwareConcurrency
      }
      
      async function start() {
        console.log("Downloading model... (see network tab for progress)");
        let modelPath = 'https://huggingface.co/rocca/lit-web/resolve/main/embed_text_tokens.onnx';
        const session = await ort.InferenceSession.create(modelPath, { executionProviders: ["wasm"] });
        console.log("Model loaded.");
        
        let texts = ["hello world!", "testing one two three", "hi", "aaaaaaaaaaaaaaaa"];
        // TODO: Add the tokenizer here. Waiting for response here: https://twitter.com/rocca27/status/1525406593617391617
        
        // Tokens computed in Python notebook:
        let textTokens = [[101,7592,2088,999,0,0,0,0,0,0,0,0,0,0,0,0],[101,5604,2028,2048,2093,0,0,0,0,0,0,0,0,0,0,0],[101,7632,0,0,0,0,0,0,0,0,0,0,0,0,0,0],[101,13360,11057,11057,11057,11057,11057,11057,2050,0,0,0,0,0,0,0]];
        
        textTokens = Int32Array.from(textTokens[0]); // just use the first one for this test ("hello world!")
        
        const feeds = {'text_tokens': new ort.Tensor('int32', textTokens, [1,16])};

        console.log("Running inference...");
        const results = await session.run(feeds);
        console.log("Finished inference.");

        const data = results["Identity_1:0"].data;
        console.log(`data of result tensor 'output'`, data); // Should match this Python output: <tf.Tensor: shape=(1, 768), dtype=float32, numpy= array([[-5.62530532e-02,  6.09267130e-02, -3.17819528e-02, 4.48665069e-03, -3.58026102e-02,  6.82989089e-03, 9.78150684e-03,  1.03076296e-02,  5.18161384e-03, 3.06973103e-02, -2.85994262e-02, -2.11724918e-02, -4.90405224e-03, -9.76243801e-03, -2.13515703e-02, 1.59902871e-02,  1.87387168e-02,  2.60726195e-02, 6.17518760e-02, -1.82616264e-02, -2.21678484e-02, 1.55742103e-02, -8.61097034e-03, -1.34193553e-02, ..., 3.38867083e-02, -2.82331626e-03, -4.84273173e-02, -7.01147467e-02, -6.88162120e-03, -3.05738044e-03]], dtype=float32)>
      }

    </script>
  </body>
</html>
